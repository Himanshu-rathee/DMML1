checkresiduals(gdp.3.holt.d)
ddgdp.2
checkresiduals(ddgdp.2)
par(mfrow = c(2,2))
# checking the plots - examining the error, trend, seasonality
plot(gdp, main = "Normal")
library(fpp2)
# smotthing the plot for checking Moving average
plot(ma(gdp,3), main = "q = 3")
plot(ma(gdp,5), main = "q = 5")
plot(ma(gdp,7), main = "q = 7")
# Models Expential Smoothing
par(mfrow = c(1,1))
# Holts Linear Trend Model 2 (level + trend)-
gdp.2.ets <- ets(gdp,model = "AAN")
gdp.2.ets
round(accuracy(gdp.2.ets),3)
# Holts Model with damped Trend Model 3 (level + trend + damped)-
gdp.3.holt.d <- holt(gdp, h = 3, damped = TRUE, PI=FALSE)
gdp.3.holt.d
round(accuracy(gdp.3.holt.d),3) ######------BEST ETS-----######
# Auto ETS method Model
gdp.ets <- ets(gdp, model = "ZZZ")
gdp.ets
round(accuracy(gdp.ets),3)
autoplot(gdp)
# checking best number of difference in time series
ndiffs(gdp) # 2
# diff = 1
dgdp <- diff(gdp)
autoplot(dgdp)
# checking stationarity
library(tseries)
adf.test(dgdp) # not stationary
autoplot(dgdp)
# Chossing p and q value
ggtsdisplay(ddgdp)
# Chossing p and q value
ggtsdisplay(ddgdp, main = "asdf")
# Chossing p and q value
ggtsdisplay(ddgdp, main = "Difference = 2")
# Checking auto ARIMA
ddgdp.3.auto <- auto.arima(gdp)
ddgdp.3.auto
# Auto ETS method Model
gdp.ets <- ets(gdp, model = "ZZZ")
gdp.ets
# Holts Linear Trend Model 2 (level + trend)-
gdp.2.ets <- ets(gdp,model = "AAN")
gdp.2.ets
round(accuracy(gdp.2.ets),3)
# checking best number of difference in time series
ndiffs(gdp) # 2
adf.test(dgdp) # not stationary
gdp.3.holt.d
# Holts Model with damped Trend Model 3 (level + trend + damped)-
gdp.3.holt.d <- holt(gdp, h = 3, damped = TRUE, PI=FALSE)
gdp.3.holt.d
round(accuracy(gdp.3.holt.d),3) ######------BEST ETS-----######
round(accuracy(gdp.3.holt.d),2) ######------BEST ETS-----######
round(accuracy(ddgdp.2),2) #####-------BEST---------############
ddgdp.2
summary(ddgdp.2)
checkresiduals(ddgdp.2)
# Evaluating model
par(mfrow = c(1,1))
qqnorm(ddgdp.2$residuals,main = "Arima (p,d,q) = (4,2,0)")
qqline(ddgdp.2$residuals)
Box.test(ddgdp.2, type = "Ljung-Box")
Box.test(ddgdp.2$residuals, type = "Ljung-Box")
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/knn/")
source("./../bank_import_primaryExplore.R")
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/knn/")
source("./../bank_import_primaryExplore.R")
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/knn/")
source("./../bank_import_primaryExplore.R")
library(dplyr)
library(class)
library(fastDummies)
# Method 2 - used scale inbulit R method to scale the method at place of normalization using z score
bank_n_cont <- as.data.frame(scale(bank[c(1,11,12,13,14,16,17,18,19,20)]))
summary(bank_n_cont)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/9_DecisionTree/ML_R")
remove(list = ls())
# Step 1 : Collecting data
credit <- read.csv("credit.csv")
# Step 1 : Collecting data
credit <- read.csv("credit.csv")
# Step 2 : Exploring and preparing the data
str(credit)
summary(credit)
table(credit$default)
# Data preparation
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
# creating train & test data sets
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
library(C50)
# Step 3 : Training a model on the data
credit_model <- C50::C5.0(credit_train[-17], credit_train$default)
credit_model
summary(credit_model)
library(caret)
# Step 4 : Evaluating model performance
credit_predict <- predict(object = credit_model, newdata = credit_test)
caret::confusionMatrix(credit_predict, credit_test$default)
# Step 5 : Improving model performance
credit_boost10 <- C50::C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_model10
credit_boost10
summary(credit_boost10)
source('~/.active-rstudio-document', echo=TRUE)
credit_boost10
credit_boost_predict10 <- predict(object = credit_model, newdata = credit_test)
caret::confusionMatrix(credit_boost_predict10, credit_test$default)
caret::confusionMatrix(credit_predict, credit_test$default)
credit_boost_predict10 <- predict(object = credit_boost10, newdata = credit_test)
caret::confusionMatrix(credit_boost_predict10, credit_test$default)
# Adding mistakes more costlier than others
mtr_dim <- list(c("no", "yes"), c("no","yes"))
names(mtr_dim) <- c("predict","actual")
mtr_dim
err_cst <- matrix(c(0,1,0,4), nrow = 2, dimnames = mtr_dim)
err_cst
# Model Costs
credit_cost <- C50::C5.0(credit_train[-17], credit_train$default, costs = err_cst)
# Model Costs
credit_cost <- C50::C5.0(credit_train[-17], credit_train$default, costs = err_cst)
credit_cost_predict <- predict(object = credit_cost, newdata = credit_test)
caret::confusionMatrix(credit_cost_predict, credit_test$default)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/9_DecisionTree/ML_R")
remove(list = ls())
# Step 1 : Collecting data
mushrooms <- read.csv("mushrooms.csv")
# Step 2 : Exploring and preparing the data
str(mushrooms)
summary(mushrooms)
mushrooms$veil_type <- NULL
table(mushrooms$type)
install.packages("RWeka")
library(RWeka)
library(caret)
# Step 3 : Training a model on the data
mushroom_1R <- OneR(type ~ ., data = mushroom_1R)
# Step 3 : Training a model on the data
mushroom_1R <- OneR(type ~ ., data = mushrooms)
summary(mushroom_1R)
mushroom_1R
# Step 5 : Improving model performance
# RIPPER
mushroom_JRip <- JRip(type ~ . , data = mushrooms)
mushroom_JRip
summary(mushroom_JRip)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/Decision_tree/")
source("./../bank_import_primaryExplore.R")
library(C50)
library(caret)
# creating training and testing dataset from exisitng sample
indx <- createDataPartition(bank$y, p = 0.8, list = FALSE)
bank_train <- bank_n[indx,]
bank_test <- bank_n[- indx,]
bank_train <- bank[indx,]
bank_test <- bank[- indx,]
prop.table(table(bank_train$y))
prop.table(table(bank_test$y))
# Step 3 : Training a model on the data
bank_model <- C50::C5.0(bank_train[-21],bank_train$y
# Step 3 : Training a model on the data
bank_model <- C50::C5.0(bank_train[-21],bank_train$y)
# Step 3 : Training a model on the data
bank_model <- C50::C5.0(bank_train[-21],bank_train$y)
bank_model
summary(bank_model)
# Step 4 : Evaluating model performance
bank_predict <- predict(object = bank_model, newdata = bank_test)
caret::confusionMatrix(bank_predict, bank_predict$y)
caret::confusionMatrix(bank_predict, bank_test$y)
# Step 5 : Improving model performance
bank_boost10 <- C50::C5.0(bank_train[-17], bank_train$y, trials = 10)
bank_boost10
summary(bank_boost10)
bank_boost_predict10 <- predict(object = bank_boost10, newdata = bank_test)
caret::confusionMatrix(bank_boost_predict10, bank_test$default)
caret::confusionMatrix(bank_boost_predict10, bank_test$y)
# Step 5 : Improving model performance
bank_boost10 <- C50::C5.0(bank_train[-21], bank_train$y, trials = 10)
bank_boost10
summary(bank_boost10)
bank_boost_predict10 <- predict(object = bank_boost10, newdata = bank_test)
caret::confusionMatrix(bank_boost_predict10, bank_test$y)
caret::confusionMatrix(bank_predict, bank_test$y)
#Method 2 -
bank_boost5 <- C50::C5.0(bank_train[-21], bank_train$y, trials = 5)
bank_boost5
summary(bank_boost5)
bank_boost_predict5 <- predict(object = bank_boost5, newdata = bank_test)
caret::confusionMatrix(bank_boost_predict5, bank_test$y)
# Method 3 -
# Adding mistakes more costlier than others
mtr_dim <- list(c("no", "yes"), c("no","yes"))
names(mtr_dim) <- c("predict","actual")
mtr_dim
err_cst <- matrix(c(0,1,0,4), nrow = 2, dimnames = mtr_dim)
err_cst
# Model Costs
bank_cost <- C50::C5.0(bank_train[-21], bank_train$y, costs = err_cst)
bank_cost_predict <- predict(object = bank_cost, newdata = bank_test)
caret::confusionMatrix(bank_cost_predict, bank_test$y)
err_cst <- matrix(c(0,1,0,8), nrow = 2, dimnames = mtr_dim)
err_cst
# Model Costs
bank_cost <- C50::C5.0(bank_train[-21], bank_train$y, costs = err_cst)
bank_cost_predict <- predict(object = bank_cost, newdata = bank_test)
caret::confusionMatrix(bank_cost_predict, bank_test$y)
caret::confusionMatrix(bank_predict, bank_test$y)
caret::confusionMatrix(bank_cost_predict, bank_test$y)
caret::confusionMatrix(bank_boost_predict5, bank_test$y)
caret::confusionMatrix(bank_predict, bank_test$y)
caret::confusionMatrix(bank_cost_predict, bank_test$y)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/9_DecisionTree/ML_R")
remove(list = ls())
library(C50)
library(caret)
# Step 1 : Collecting data
credit <- read.csv("credit.csv")
# Step 2 : Exploring and preparing the data
str(credit)
summary(credit)
table(credit$default)
# Data preparation
set.seed(123)
train_sample <- sample(1000, 900)
str(train_sample)
# creating train & test data sets
credit_train <- credit[train_sample,]
credit_test <- credit[-train_sample,]
prop.table(table(credit_train$default))
prop.table(table(credit_test$default))
# Step 3 : Training a model on the data
credit_model <- C50::C5.0(credit_train[-17], credit_train$default)
credit_model
summary(credit_model)
# Step 4 : Evaluating model performance
credit_predict <- predict(object = credit_model, newdata = credit_test)
caret::confusionMatrix(credit_predict, credit_test$default)
# Step 5 : Improving model performance
credit_boost10 <- C50::C5.0(credit_train[-17], credit_train$default, trials = 10)
credit_boost10
summary(credit_boost10)
credit_boost_predict10 <- predict(object = credit_boost10, newdata = credit_test)
caret::confusionMatrix(credit_boost_predict10, credit_test$default)
# Adding mistakes more costlier than others
mtr_dim <- list(c("no", "yes"), c("no","yes"))
names(mtr_dim) <- c("predict","actual")
mtr_dim
err_cst <- matrix(c(0,1,0,4), nrow = 2, dimnames = mtr_dim)
err_cst
# Model Costs
credit_cost <- C50::C5.0(credit_train[-17], credit_train$default, costs = err_cst)
credit_cost_predict <- predict(object = credit_cost, newdata = credit_test)
caret::confusionMatrix(credit_cost_predict, credit_test$default)
caret::confusionMatrix(credit_predict, credit_test$default)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/Decision_tree/")
source("./../bank_import_primaryExplore.R")
library(C50)
library(caret)
# creating training and testing dataset from exisitng sample
indx <- createDataPartition(bank$y, p = 0.8, list = FALSE)
bank_train <- bank[indx,]
bank_test <- bank[- indx,]
prop.table(table(bank_train$y))
prop.table(table(bank_test$y))
# Step 3 : Training a model on the data
bank_model <- C50::C5.0(bank_train[-21],bank_train$y)
bank_model
summary(bank_model)
# Step 4 : Evaluating model performance
bank_predict <- predict(object = bank_model, newdata = bank_test)
caret::confusionMatrix(bank_predict, bank_test$y)
# Step 5 : Improving model performance
# Method 1 -
bank_boost10 <- C50::C5.0(bank_train[-21], bank_train$y, trials = 10)
bank_boost10
summary(bank_boost10)
bank_boost_predict10 <- predict(object = bank_boost10, newdata = bank_test)
caret::confusionMatrix(bank_boost_predict10, bank_test$y)
# Method 2 -
bank_boost5 <- C50::C5.0(bank_train[-21], bank_train$y, trials = 5)
bank_boost5
summary(bank_boost5)
bank_boost_predict5 <- predict(object = bank_boost5, newdata = bank_test)
caret::confusionMatrix(bank_boost_predict5, bank_test$y)
# Method 3 -
# Adding mistakes more costlier than others
mtr_dim <- list(c("no", "yes"), c("no","yes"))
names(mtr_dim) <- c("predict","actual")
mtr_dim
err_cst <- matrix(c(0,1,0,100), nrow = 2, dimnames = mtr_dim)
err_cst
# Model Costs
bank_cost <- C50::C5.0(bank_train[-21], bank_train$y, costs = err_cst)
bank_cost_predict <- predict(object = bank_cost, newdata = bank_test)
caret::confusionMatrix(bank_cost_predict, bank_test$y)
err_cst <- matrix(c(0,1,0,4), nrow = 2, dimnames = mtr_dim)
err_cst
# Model Costs
bank_cost <- C50::C5.0(bank_train[-21], bank_train$y, costs = err_cst)
bank_cost_predict <- predict(object = bank_cost, newdata = bank_test)
caret::confusionMatrix(bank_cost_predict, bank_test$y)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/Decision_tree/")
source("./../bank_import_primaryExplore.R")
library(C50)
library(caret)
library(RWeka)
# creating training and testing dataset from exisitng sample
indx <- createDataPartition(bank$y, p = 0.8, list = FALSE)
bank_train <- bank[indx,]
bank_test <- bank[- indx,]
prop.table(table(bank_train$y))
prop.table(table(bank_test$y))
# Step 3 : Training a model on the data
bank_1R <- OneR(y ~ ., data = bank_train)
bank_1R
summary(bank_1R)
# Step 4 : Evaluating model performance
bank_predict_1R <- predict(object = bank_1R, newdata = bank_test)
caret::confusionMatrix(bank_predict_1R, bank_test$y)
# Step 5 : Improving model performance
# RIPPER
bank_JRip <- JRip(y ~ . , data = bank_test)
bank_JRip
summary(bank_JRip)
bank_predict_JRip <- predict(object = bank_JRip, newdata = bank_test)
caret::confusionMatrix(bank_predict_JRip, bank_test$y)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Kc_house/")
source("Kc_Import_Explore_Clean.R")
# Step 1 : Collecting data
# Step 1 : Collecting data
# Step 2 : Explore Data & data prepration
house_train <- kc_house[indx,]
house_test <- kc_house[- indx,]
h
# creating training and testing dataset from exisitng sample
indx <- createDataPartition(kc_house$price, p = 0.8, list = FALSE)
house_train <- kc_house[indx,]
house_test <- kc_house[- indx,]
install.packages("rpart")
install.packages("rpart")
library(rpart)
# Step 3 : Training a model
house.rpart <- rpart(price ~ ., data = house_train)
house.rpart
summary(house.rpart)
summary(house.rpart)
install.packages("rpart.plot")
library(rpart.plot)
# visulazing decision tree
rpart.plot(house.rpart, digits = 3)
# visulazing decision tree
rpart.plot(house.rpart, digits = 4)
# visulazing decision tree
rpart.plot(house.rpart, digits = 3)
# visulazing decision tree
rpart.plot(house.rpart, digits = 3, fallen.leaves = TRUE)
# visulazing decision tree
rpart.plot(house.rpart, digits = 3, fallen.leaves = TRUE, extra = 101)
# visulazing decision tree
rpart.plot(house.rpart, digits = 3)
# Step 4 : Evaluating model performace
house.predict.rpart <- predict(house.rpart, house_test)
summary(house.predict.rpart)
summary(house_test$price)
# checking correlation
cor((house.predict.rpart, house_test$price)
# checking correlation
cor(house.predict.rpart, house_test$price)
# cheking performace with MAE
mae <- function(actual, pred) {
mean(abs(actual-pred))
}
mae(house_test$price, house.predict.rpart)
mean(house_train$price)
mean(house_test$price)
mae(house_test$price, house.predict.rpart)
mae(mae(mean(house_train$price), house.predict.rpart))
mae(mean(house_train$price), house.predict.rpart))
mae(mean(house_train$price), house.predict.rpart)
library(RWeka)
library(rpart)
library(rpart.plot)
# Step 5 : Improving model
# model tree
house.m5p <- M5P(price ~ . , data = kc_house)
house.m5p
summary(house.m5p)
# Step 5 : Improving model
# model tree
house.m5p <- M5P(price ~ . , data = house_train)
house.m5p
summary(house.m5p)
house.predict.m5p <- predict(house.m5p, house_test)
summary(house.predict.m5p)
cor(house.predict.m5p, house_test$price)
mae(house_test$price, house.predict.m5p)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/Decision_tree/")
source("./../bank_import_primaryExplore.R")
library(caret)
# creating training and testing dataset from exisitng sample
indx <- createDataPartition(bank$y, p = 0.8, list = FALSE)
bank_train <- bank[indx,]
bank_test <- bank[- indx,]
prop.table(table(bank_train$y))
prop.table(table(bank_test$y))
install.packages("kernlab")
library(kernlab)
# Step 3 : Training a model on the data
bank_svm <- ksvm(y ~ ., data = bank_train, kernel = "vanilladot")
bank_svm
summary(bank_svm)
# Step 4 : Evaluating model performance
bank_predict_svm <- predict(object = bank_svm, newdata = bank_test)
caret::confusionMatrix(bank_predict_svm, bank_test$y)
kernlab::plot(bank_svm)
# Step 2 : Exploring and preparing the data
# Done in source above
kernlab::plot(bank$y)
x <- matrix(rnorm(20 * 2), ncol = 2)
y <- c(rep(-1, 10), rep(1, 10))
x[y == 1,] <- x[y == 1,] + 1
plot(x, col = (3 - y))
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/Decision_tree/")
source("./../bank_import_primaryExplore.R")
library(caret)
library(kernlab)
# Step 2 : Exploring and preparing the data
# Done in source above
kernlab::sc(bank$y)
# creating training and testing dataset from exisitng sample
indx <- createDataPartition(bank$y, p = 0.8, list = FALSE)
bank_train <- bank[indx,]
bank_test <- bank[- indx,]
prop.table(table(bank_train$y))
prop.table(table(bank_test$y))
# Step 3 : Training a model on the data
bank_svm <- ksvm(y ~ ., data = bank_train, kernel = "vanilladot")
bank_svm
summary(bank_svm)
# Step 4 : Evaluating model performance
bank_predict_svm <- predict(object = bank_svm, newdata = bank_test)
caret::confusionMatrix(bank_predict_svm, bank_test$y)
# Step 5 : Improving model performance
# RIPPER
bank_svm.rbf <- ksvm(y ~ . , data = bank_test, kernel = "rbfdot")
bank_svm.rbf
# Step 5 : Improving model performance
# RIPPER
bank_svm.rbf <- ksvm(y ~ . , data = bank_train, kernel = "rbfdot")
bank_svm.rbf
summary(bank_svm.rbf)
bank_predict_svm.rbf <- predict(object = bank_svm.rbf, newdata = bank_test)
caret::confusionMatrix(bank_predict_svm.rbf, bank_test$y)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/Project/Bank/Decision_tree/")
source("./../bank_import_primaryExplore.R")
library(C50)
library(caret)
library(RWeka)
# creating training and testing dataset from exisitng sample
indx <- createDataPartition(bank$y, p = 0.8, list = FALSE)
bank_train <- bank[indx,]
bank_test <- bank[- indx,]
prop.table(table(bank_train$y))
prop.table(table(bank_test$y))
# Step 3 : Training a model on the data
bank_1R <- OneR(y ~ ., data = bank_train)
bank_1R
# Step 5 : Improving model performance
# RIPPER
bank_JRip <- JRip(y ~ . , data = bank_train)
bank_JRip
summary(bank_JRip)
bank_predict_JRip <- predict(object = bank_JRip, newdata = bank_test)
caret::confusionMatrix(bank_predict_JRip, bank_test$y)
# Method 2 : kernel = polydot
bank_svm.poly <- ksvm(y ~ . , data = bank_train, kernel = "polydot")
bank_predict_svm.poly <- predict(object = bank_svm.poly, newdata = bank_test)
caret::confusionMatrix(bank_predict_svm.poly, bank_test$y)
# Method 3 : kernel = tanhdot
bank_svm.tanh <- ksvm(y ~ . , data = bank_train, kernel = "tanhdot")
bank_predict_svm.tanh <- predict(object = bank_svm.tanh, newdata = bank_test)
caret::confusionMatrix(bank_predict_svm.tanh, bank_test$y)
# from above 3 methods - trying adding cost param in kernel = rbfdot
# Method 4 : kernel = rbfdot & C = 10
bank_svm.rbf10 <- ksvm(y ~ . , data = bank_train, kernel = "rbfdot", c = 10)
# from above 3 methods - trying adding cost param in kernel = rbfdot
# Method 4 : kernel = rbfdot & C = 10
bank_svm.rbf10 <- ksvm(y ~ . , data = bank_train, kernel = "rbfdot", C = 10)
bank_svm.rbf10
summary(bank_svm.rbf10)
bank_predict_svm.rbf10 <- predict(object = bank_svm.rbf10, newdata = bank_test)
caret::confusionMatrix(bank_predict_svm.rbf10, bank_test$y)
# Method 5 : kernel = rbfdot & C = 50
bank_svm.rbf50 <- ksvm(y ~ . , data = bank_train, kernel = "rbfdot", C = 50)
bank_predict_svm.rbf50 <- predict(object = bank_svm.rbf50, newdata = bank_test)
caret::confusionMatrix(bank_predict_svm.rbf50, bank_test$y)
# Method 6 : kernel = rbfdot & C = 50
bank_svm.rbf100 <- ksvm(y ~ . , data = bank_train, kernel = "rbfdot", C = 100)
bank_predict_svm.rbf100 <- predict(object = bank_svm.rbf100, newdata = bank_test)
caret::confusionMatrix(bank_predict_svm.rbf100, bank_test$y)
