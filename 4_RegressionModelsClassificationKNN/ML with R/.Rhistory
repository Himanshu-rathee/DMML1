titanicData$Child[titanicData$Age < 18] <- "Y"
titanicData$Child[titanicData$Age >= 18] <- "N"
titanicData$Child <- as.factor(titanicData$Child)
# Create the plot below with the new Child column.
ggplot(titanicData, aes(Child, fill = factor(Survived))) + geom_bar(stat='count', position='dodge') + facet_grid(.~Sex) +
theme_few()
# Make the plot below, to help us understand if Fsize has any rela- tionship with survival
ggplot(titanicData, aes(x = Fsize, fill = factor(Survived))) + geom_bar(stat='count', position='dodge') + scale_x_continuous(breaks=c(1:11)) +
labs(x = 'Family Size') +
theme_few()
# Create a new factor called Child
titanicData$Child[titanicData$Age < 18] <- "Y"
titanicData$Child[titanicData$Age >= 18] <- "N"
titanicData$Child <- as.factor(titanicData$Child)
# Create the plot below with the new Child column.
ggplot(titanicData, aes(Child, fill = factor(Survived))) + geom_bar(stat='count', position='dodge') + facet_grid(.~Sex) +
theme_few()
# Make the plot below, to help us understand if Fsize has any rela- tionship with survival
ggplot(titanicData, aes(x = Fsize, fill = factor(Survived))) + geom_bar(stat='count', position='dodge') + scale_x_continuous(breaks=c(1:11)) +
labs(x = 'Family Size') +
theme_few()
# Create the plot below with the new Child column.
ggplot(titanicData, aes(Child, fill = factor(Survived))) + geom_bar(stat='count', position='dodge') + facet_grid(.~Sex) +
theme_few()
source('~/.active-rstudio-document', echo=TRUE)
View(bank)
bank <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/bank-additional-full.csv", sep = ";")
View(bank)
bank <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/bank-additional-full.csv", sep = ";", header = TRUE)
View(bank)
View(bank)
str(bank)
lm_fit <- lm(y~duration, data = bank)
summary(lm_fit)
View(bank)
table(bank$y)
bank.new <- bank[which(bank$duration != 0)]
which(bank$duration != 0)
which(bank$duration == 0)
bank.new <- bank[which(bank$duration != 0),]
bank[c(which(bank$duration == 0))]
bank[c(which(bank$duration == 0)),]
bank[c(which(bank$duration > 0)),]
bank[c(which(bank$duration < 0)),]
bank[c(which(bank$duration <= 0)),]
plot(table(bank$y))
hist(bank$y)
hist(table(bank$y))
table(bank$y)
plot(bank$y)
str(bank)
plot(bank$y)
table(bank$y)
# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
# For example, here's several helpful packages to load in
bank <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/bank-additional-full.csv", sep = ";", header = TRUE)
str(bank)
table(bank$y)
plot(bank$y)
#number of numeric variables
x<-sapply(bank,is.numeric)
bank_numeric<-bank[,x]
head(bank_numeric)
#number of factor variables
y<-sapply(bank, is.factor)
bank_factor<-bank[,y]
head(bank_factor)
#missing values treatment
library(Amelia)
missmap(bank, y.at = 1,y.labels = "",col=c("red","black"),legend = FALSE)
sum(is.na(bank))
#age outliers
boxplot(bank$age)
#age outlier capping
upper_side_outliers_age <- quantile(bank$age, 0.75) + 1.5*IQR(bank$age)
bank[bank$age > round(upper_side_outliers_age), "age"] <- round(upper_side_outliers_age)
min(bank$age)
max(bank$age)
bank$age<-ifelse(bank$age<=18 & bank$age<40,"adult",ifelse(bank$age>=40 & bank$age<58,
"middle age","older"))
bank$age<-as.factor(bank$age)
#duration outliers
bank$duration<-(bank$duration/60)
boxplot(bank$duration)
#duration outlier capping
upper_side_outliers_duration <- quantile(bank$duration, 0.75) + 1.5*IQR(bank$duration)
bank[bank$duration > round(upper_side_outliers_duration), "duration"] <- round(upper_side_outliers_duration)
min(bank$duration)
max(bank$duration)
boxplot(bank$duration)
bank$duration<-ifelse(bank$duration<5 ,"1",ifelse(bank$duration>=5 & bank$duration<10,"2",
ifelse(bank$duration>=10 & bank$duration<15,"3","4")))
bank$duration<-as.factor(bank$duration)
#EDA
library(ggplot2)
ggplot(bank,aes(age,fill=deposit))+geom_bar()+
ggtitle("Age vs Deposit")->p1
p1
ggplot(bank,aes(age,fill=y))+geom_bar()+
ggtitle("Age vs Deposit")->p1
p1
bank$deposit <- bank$y
bank <- bank[,-y]
View(bank)
# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
# For example, here's several helpful packages to load in
bank <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/bank-additional-full.csv", sep = ";", header = TRUE)
str(bank)
table(bank$y)
plot(bank$y)
bank$deposit <- bank$y
str(bank)
bank <- bank[,-y]
str(bank)
# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
# For example, here's several helpful packages to load in
bank <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/bank-additional-full.csv", sep = ";", header = TRUE)
str(bank)
table(bank$y)
plot(bank$y)
bank$deposit <- bank$y
bank <- bank[,c(-y)]
str(bank)
# This R environment comes with all of CRAN preinstalled, as well as many other helpful packages
# The environment is defined by the kaggle/rstats docker image: https://github.com/kaggle/docker-rstats
# For example, here's several helpful packages to load in
bank <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/bank-additional-full.csv", sep = ";", header = TRUE)
str(bank)
table(bank$y)
plot(bank$y)
bank$deposit <- bank$y
bank$y <- NULL
str(bank)
#number of numeric variables
x<-sapply(bank,is.numeric)
bank_numeric<-bank[,x]
head(bank_numeric)
#number of factor variables
y<-sapply(bank, is.factor)
bank_factor<-bank[,y]
head(bank_factor)
#missing values treatment
library(Amelia)
missmap(bank, y.at = 1,y.labels = "",col=c("red","black"),legend = FALSE)
sum(is.na(bank))
#age outliers
boxplot(bank$age)
#age outlier capping
upper_side_outliers_age <- quantile(bank$age, 0.75) + 1.5*IQR(bank$age)
bank[bank$age > round(upper_side_outliers_age), "age"] <- round(upper_side_outliers_age)
min(bank$age)
max(bank$age)
bank$age<-ifelse(bank$age<=18 & bank$age<40,"adult",ifelse(bank$age>=40 & bank$age<58,
"middle age","older"))
bank$age<-as.factor(bank$age)
#duration outliers
bank$duration<-(bank$duration/60)
boxplot(bank$duration)
#duration outlier capping
upper_side_outliers_duration <- quantile(bank$duration, 0.75) + 1.5*IQR(bank$duration)
bank[bank$duration > round(upper_side_outliers_duration), "duration"] <- round(upper_side_outliers_duration)
min(bank$duration)
max(bank$duration)
boxplot(bank$duration)
bank$duration<-ifelse(bank$duration<5 ,"1",ifelse(bank$duration>=5 & bank$duration<10,"2",
ifelse(bank$duration>=10 & bank$duration<15,"3","4")))
bank$duration<-as.factor(bank$duration)
#EDA
library(ggplot2)
ggplot(bank,aes(age,fill=deposit))+geom_bar()+
ggtitle("Age vs Deposit")->p1
p1
ggplot(bank,aes(job,fill=deposit))+geom_bar()+
ggtitle("Job vs Deposit")->p2
p2
ggplot(bank,aes(marital,fill=deposit))+geom_bar()+
ggtitle("Marital vs Depsoit")->p3
p3
ggplot(bank,aes(education,fill=deposit))+geom_bar()+
ggtitle("Education vs Deposit")->p4
p4
ggplot(bank,aes(default,fill=deposit))+geom_bar()+
ggtitle("Default vs Deposit")->p5
p5
ggplot(bank,aes(balance))+geom_histogram(aes(fill=deposit),color="black")+
ggtitle("Balance vs Deposit")->p6
p6
ggplot(bank,aes(housing,fill=deposit))+geom_bar()+
ggtitle("Housing vs Deposit")->p7
p7
ggplot(bank,aes(loan,fill=deposit))+geom_bar()+
ggtitle("Loan vs Deposit")->p8
p8
ggplot(bank,aes(contact,fill=deposit))+geom_bar()+
ggtitle("Contact vs Deposit")->p9
p9
ggplot(bank,aes(month,fill=deposit))+geom_bar()+
ggtitle("Month vs Deposit")->p10
p10
ggplot(bank,aes(campaign))+geom_histogram(aes(fill=deposit),color="black",binwidth =5)+
ggtitle("Campaign vs Deposit")->p11
p11
ggplot(bank,aes(duration,fill=deposit))+geom_bar()+
ggtitle("Duration vs Deposit")->p12
p12
library(gridExtra)
grid.arrange(p1,p2,p3)->g1
g1
grid.arrange(p4,p5,p6)->g2
g2
grid.arrange(p7,p8,p9)->g3
g3
grid.arrange(p10,p11,p12)->g4
g4
#correaltion matrix
library(corrplot)
library(psych)
bank_cor <- bank
for(i in 1:ncol(bank_cor)){
bank_cor[,i]<- as.integer(bank_cor[,i])
}
corrplot(cor(bank_cor))
#outliers
boxplot(bank$balance)
upper_side_outliers_balance <- quantile(bank$balance, 0.75) + 1.5*IQR(bank$balance)
#Capping was done on this part
bank[bank$balance > round(upper_side_outliers_balance), "balance"] <- round(upper_side_outliers_balance)
#base accuracy
prop.table(table(bank$deposit))
bank$deposit<-ifelse(bank$deposit=="yes",1,0)
#splitting the data
library(caTools)
set.seed(1234)
split <- sample.split(bank$deposit, SplitRatio = 0.7)
train <- subset(bank, split == TRUE)
test <- subset(bank, split == FALSE)
#model bulding without removing correlated variables
log.model1 <- glm(deposit ~ ., data=train, family = binomial(link='logit'))
summary(log.model1)
## coefficients
exp(coef(log.model1))
#prediction
pred1 <- predict(log.model1, newdata=test, type = "response")
table(test$deposit, pred1>= 0.5)
(1513+1219)/nrow(test)
#confusion matrix
library(caret)
pred_threshold1<-ifelse(pred1>=0.5,1,0)
confusionMatrix(pred_threshold1,test$deposit)
# Area under the curve
library(ROCR)
predic1<-prediction(pred1, test$deposit)
# creating ROC curve
roc1<-performance(predic1,"tpr","fpr")
plot(roc1)
title("ROC Curve")
auc1<- performance(predic1, measure = "auc")
auc1 <- auc1@y.values[[1]]
auc1
#cross validation
ctrl1 <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
mod_fit1 <- train(deposit ~.,  data=bank, method="glm", family="binomial",
trControl = ctrl1, tuneLength = 5)
summary(mod_fit1)
pred_cross1 = predict(mod_fit1, newdata=test)
pred_cross1<-ifelse(pred_cross1>=0.5, 1,0)
confusionMatrix(pred_cross1, test$deposit)
#variable importance
varImp(log.model1,scale=FALSE)
#building model without correlated variables
log.model2<-glm(deposit~age+job+marital+education+default+balance+housing+
loan+contact+day+ month + duration + campaign,
data=train, family = binomial(link = 'logit'))
summary(log.model2)
## coefficients
exp(coef(log.model2))
#prediction
pred2 <- predict(log.model2, newdata=test, type = "response")
table(test$deposit, pred2 >= 0.5)
(1474+1223)/nrow(test)
#confusion matrix
pred_threshold2<-ifelse(pred2>=0.5,1,0)
confusionMatrix(pred_threshold2,test$deposit)
# Area under the curve
predic2<-prediction(pred2, test$deposit)
# creating ROC curve
roc2<-performance(predic2,"tpr","fpr")
plot(roc2)
title("ROC Curve")
auc2 <- performance(predic2, measure = "auc")
auc2 <- auc2@y.values[[1]]
auc2
#cross validation
ctrl2 <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)
mod_fit2 <- train(deposit~age+job+education+default+balance+housing+
loan+contact+day+ month + duration + campaign,  data=bank, method="glm", family="binomial",
trControl = ctrl2, tuneLength = 5)
summary(mod_fit2)
pred_cross2 = predict(mod_fit2, newdata=test)
pred_cross2<-ifelse(pred_cross2>=0.5, 1,0)
confusionMatrix(pred_cross2, test$deposit)
#variable importance
varImp(log.model2,scale=FALSE)
#model1 has more acccuracy  than model2
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model 1 has more accuracy than model 2 in cross validation
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model 1 has more accuracy than model 2 in cross validation
#model 1 has lower aic value than model 2 in cross validation
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model 1 has more accuracy than model 2 in cross validation
#model 1 has lower aic value than model 2 in cross validation
#so model 1 is better than model 2.
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model 1 has more accuracy than model 2 in cross validation
#model 1 has lower aic value than model 2 in cross validation
#so model 1 is better than model 2.
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model 1 has more accuracy than model 2 in cross validation
#model 1 has lower aic value than model 2 in cross validation
#so model 1 is better than model 2.
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model 1 has more accuracy than model 2 in cross validation
#model 1 has lower aic value than model 2 in cross validation
#so model 1 is better than model 2.
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model 1 has more accuracy than model 2 in cross validation
#model 1 has lower aic value than model 2 in cross validation
#so model 1 is better than model 2.
#model1 has more acccuracy  than model2
#model 1 has has higher auc vlaue than model 2
#model 1 has more accuracy than model 2 in cross validation
#model 1 has lower aic value than model 2 in cross validation
#so model 1 is better than model 2.
bank <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/bank-additional-full.csv", sep = ";", header = TRUE)
str(bank)
bank.new <- bank[which(bank$duration != 0),]
which(bank$duration == 0)
bank([c(which(bank$duration <= 0)),]
plot(bank$y)
lending <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/LendingData.csv", sep = ";", header = TRUE)
lending <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/LendingData.csv", sep = ";", header = TRUE)
str(lending)
lending <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/LendingData.csv", sep = "'", header = TRUE)
lending <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/LendingData.csv", sep = ",", header = TRUE)
str(lending)
sum(is.na(lending))
print(i)
for i in lending {
print(i)
}
print(lending[,i])
print(lending[[i]])
print(i)
print(class(df[[i]]))
for i in colnames(lending) {
print(class(df[[i]]))
}
sum(is.na(lending))
for i in colnames(lending) {
print(class(df[[i]]))
}
for (i in colnames(lending)) {
print(class(df[[i]]))
}
print(class(lending[[i]]))
for (i in colnames(lending)) {
print(class(lending[[i]]))
}
for (i in colnames(lending)) {
print(lending[[i]])
}
for(i in names(lending)){
df[[paste(i, 'length', sep="_")]] <- str_length(lending[[i]])
}
df[[paste(i, 'length', sep="_")]] <- length(lending[[i]])
for(i in names(lending)){
df[[paste(i, 'length', sep="_")]] <- length(lending[[i]])
}
print(i)
#print(sum(is.na()))
}
for(i in names(lending)){
print(i)
#print(sum(is.na()))
}
print(sum(is.na(lending[,i])))
for(i in names(lending)){
print(i)
print(sum(is.na(lending[,i])))
}
print(sum(is.na(lending[,i])))
if (is.na(lending[,i])) {
print(i)
print(sum(is.na(lending[,i])))
}
for(i in names(lending)){
if (is.na(lending[,i])) {
print(i)
print(sum(is.na(lending[,i])))
}
}
str(lending)
## Importing packages
library(tidyverse)
install.packages("tidyverse")
## Importing packages
library(tidyverse)
library(caret)
library(lattice)
library(ggplot2)
library(AUC)
install.packages("AUC")
credit <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/credit_default.csv", sep = ",", header = TRUE)
book <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/books.csv", sep = ",", header = TRUE)
str(book)
plot(book$average_rating)
table(book$average_rating)
table(nrow(book))
nrow(book)
house <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/kc_house_data.csv", sep = ",", header = TRUE)
str(house)
nrow(house)
plot(house$price)
nrow(book)
plot(book$average_rating)
hist(house$price)
bank <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/bank-additional-full.csv", sep = ";", header = TRUE)
str(bank)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/4_RegressionModels/ML with R/")
# STEP 1 - COLLECTING DATA AND CHECKING THE COLUMNS/ FACTORS
# reading the file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/4_RegressionModels/ML with R/")
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/4_RegressionModels/ML with R/")
# STEP 1 - COLLECTING DATA AND CHECKING THE COLUMNS/ FACTORS
# reading the file
wbcd <- read.csv("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/4_RegressionModels/ML with R/wisc_bc_data.csv", stringsAsFactors = FALSE)
setwd("/Users/sobil/Documents/MSC/Sem 1/Data Mining & Machine Learning/4_RegressionModelsClassificationKNN/ML with R/")
# STEP 1 - COLLECTING DATA AND CHECKING THE COLUMNS/ FACTORS
# reading the file
wbcd <- read.csv("wisc_bc_data.csv", stringsAsFactors = FALSE)
# STEP 2 - EXPLORING AND PREPARING THE DATA
# removing the id field - to make the data more generalized and thus removing overfitting
wbcd <- wbcd[-1]
# checking the overall distribution of prediction/ target charcterstics/feature
table(wbcd$diagnosis)
gmodels::CrossTable(wbcd$diagnosis)
# converting the target feature into factor with adding labels for more information
wbcd$diagnosis <- factor(wbcd$diagnosis, levels = c("B", "M"), labels = c("Benign", "Malignant"))
# to just check the proportion
round(prop.table(table(wbcd$diagnosis))* 100 , digits = 1)
# checking the summary of wbcd
summary(wbcd)
# normalizing the data to standardize range of values of various characterstics
# creating normalized method
normaize <- function(x) {
return((x - min(x))/(max(x) - min(x)))
}
wbcd_n <- as.data.frame(lapply(wbcd[2:31], normaize))
summary(wbcd_n)
# creating training and testing dataset from exisitng sample
wbcd_train <- wbcd_n[1:469,]
wbcd_test <- wbcd_n[470:length(wbcd$diagnosis),]
# creating lables for test and training data sets
wbcd_train_labels <- wbcd[1:469,1]
wbcd_test_labels <- wbcd[470:length(wbcd$diagnosis),1]
# STEP 3 - TRAINING A MODEL ON THE DATA
library(class)
# using the knn method of class package with K value equalent to square root of
# total train observation & odd number to eliminate tie vote issue for 2 factor classification
wbcd_test_predict <- class::knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
# STEP 4 - EVALUATING MODEL PERFORMANCE
gmodels::CrossTable(x = wbcd_test_labels, y = wbcd_test_predict, prop.chisq = FALSE)
# STEP 5 - IMPROVING MODEL PERFORMANCE
#Method 1
# by changing k values
i <- 27
while (i > 0) {
wbcd_test_predict <- class::knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = i)
print(i)
gmodels::CrossTable(x = wbcd_test_labels, y = wbcd_test_predict, prop.chisq = FALSE)
i <- i - 1
}
# Method 2 - used scale inbulit R method to scale the method at place of normalization using z score
wbcd_z <- as.data.frame(scale(wbcd[-1]))
# creating training and testing dataset from exisitng sample
wbcd_train <- wbcd_z[1:469,]
wbcd_test <- wbcd_z[470:length(wbcd$diagnosis),]
# trainging the model again
wbcd_test_predict <- class::knn(train = wbcd_train, test = wbcd_test, cl = wbcd_train_labels, k = 21)
# evaluating
gmodels::CrossTable(x = wbcd_test_labels, y = wbcd_test_predict, prop.chisq = FALSE)
# Moreover, we should not formulate our approach too closely to our test data.
# Moreover, we should not formulate our approach too closely to our test data.
# Moreover, we should not formulate our approach too closely to our test data.
book <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/books.csv", sep = ",", header = TRUE)
str(book)
house <- read.csv("/Users/sobil/Downloads/DM_DATA_SETS/kc_house_data.csv", sep = ",", header = TRUE)
str(house)
